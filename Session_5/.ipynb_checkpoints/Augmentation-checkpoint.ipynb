{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54bb4790-6e05-4891-9b3c-4ad2eeb1b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from torchtext.legacy import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy.datasets import IMDB\n",
    "from torchtext.legacy.data import Field, LabelField, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4231df5-ef4c-4b41-8b9b-ad0242432ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f143f93d-e10c-4168-a1ef-552b29465b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch, torchtext\n",
    "import os\n",
    "import googletrans\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bd7ee-ffbd-44b7-bfce-ae31929ab40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "144bc789-c64b-49f6-9530-3ba9ca40b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanfordSentimentTreebank/SOStr.txt\n",
      "stanfordSentimentTreebank/sentiment_labels.txt\n",
      "stanfordSentimentTreebank/README.txt\n",
      "stanfordSentimentTreebank/original_rt_snippets.txt\n",
      "stanfordSentimentTreebank/datasetSplit.txt\n",
      "stanfordSentimentTreebank/dictionary.txt\n",
      "stanfordSentimentTreebank/STree.txt\n",
      "stanfordSentimentTreebank/datasetSentences.txt\n",
      "stanfordSentimentTreebank/.ipynb_checkpoints/README-checkpoint.txt\n",
      "stanfordSentimentTreebank/.ipynb_checkpoints/datasetSplit-checkpoint.txt\n",
      "stanfordSentimentTreebank/.ipynb_checkpoints/datasetSentences-checkpoint.txt\n",
      "stanfordSentimentTreebank/.ipynb_checkpoints/sentiment_labels-checkpoint.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('stanfordSentimentTreebank'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b52c8d-41ac-4b67-a2b2-d94f86c774f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_sentiments(base_directory):\n",
    "    def group_labels(label):\n",
    "        if label in [\"very negative\", \"negative\"]:\n",
    "            return \"negative\"\n",
    "        elif label in [\"positive\", \"very positive\"]:\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "\n",
    "    dictionary = pd.read_csv(os.path.join(base_directory, \"dictionary.txt\"), sep=\"|\")\n",
    "    dictionary.columns = [\"phrase\", \"id\"]\n",
    "    dictionary = dictionary.set_index(\"id\")\n",
    "\n",
    "    sentiment_labels = pd.read_csv(os.path.join(base_directory, \"sentiment_labels.txt\"), sep=\"|\")\n",
    "    sentiment_labels.columns = [\"id\", \"sentiment\"]\n",
    "    sentiment_labels = sentiment_labels.set_index(\"id\")\n",
    "\n",
    "    phrase_sentiments = dictionary.join(sentiment_labels)\n",
    "\n",
    "    phrase_sentiments[\"fine\"] = pd.cut(phrase_sentiments.sentiment, [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                           include_lowest=True,\n",
    "                                           labels=[\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"])\n",
    "    phrase_sentiments[\"coarse\"] = phrase_sentiments.fine.apply(group_labels)\n",
    "    return phrase_sentiments\n",
    "\n",
    "\n",
    "def get_sentence_partitions(base_directory):\n",
    "    sentences = pd.read_csv(os.path.join(base_directory, \"datasetSentences.txt\"), index_col=\"sentence_index\",\n",
    "                                sep=\"\\t\")\n",
    "    splits = pd.read_csv(os.path.join(base_directory, \"datasetSplit.txt\"), index_col=\"sentence_index\")\n",
    "    return sentences.join(splits)\n",
    "\n",
    "\n",
    "def partition(base_directory):\n",
    "    phrase_sentiments = get_phrase_sentiments(base_directory).reset_index(level=0)\n",
    "    sentence_partitions = get_sentence_partitions(base_directory)\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    data = sentence_partitions.join(phrase_sentiments.set_index(\"phrase\"), on=\"sentence\")\n",
    "    data[\"splitset_label\"] = data[\"splitset_label\"].fillna(1).astype(int)\n",
    "    # data[\"sentence\"] = data[\"sentence\"].str.replace(r\"\\s('s|'d|'re|'ll|'m|'ve|n't)\\b\", lambda m: m.group(1))\n",
    "    return data.groupby(\"splitset_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73336174-92bc-48b6-b6ff-8db24dec0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(sequence,lab, PROB = 1):\n",
    "    languages = ['en', 'fr', 'th', 'tr', 'ur', 'ru', 'bg', 'de', 'ar', 'zh-cn', 'hi',\n",
    "                 'sw', 'vi', 'es', 'el']\n",
    "    \n",
    "    #instantiate translator\n",
    "    translator = Translator()\n",
    "    \n",
    "    #store original language so we can convert back\n",
    "    org_lang = translator.detect(sequence).lang\n",
    "    \n",
    "    #randomly choose language to translate sequence to  \n",
    "    random_lang = np.random.choice([lang for lang in languages if lang is not org_lang])\n",
    "    #print(random_lang)\n",
    "    if org_lang in languages:\n",
    "        #translate to new language and back to original\n",
    "        translated = translator.translate(sequence, dest = random_lang).text\n",
    "        #translate back to original language\n",
    "        translated_back = translator.translate(translated, dest = org_lang).text\n",
    "        #print(translated,translated_back)\n",
    "        #apply with certain probability\n",
    "        if np.random.uniform(0, 1) <= PROB:\n",
    "            output_sequence = translated_back\n",
    "        else:\n",
    "            output_sequence = sequence\n",
    "            \n",
    "    #if detected language not in our list of languages, do nothing\n",
    "    else:\n",
    "        output_sequence = sequence\n",
    "    \n",
    "    return output_sequence,lab\n",
    "\n",
    "\n",
    "def random_deletion(words,lab, p=0.5): \n",
    "    if len(words) == 1: # return if single word\n",
    "        return words\n",
    "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
    "    \n",
    "    if len(remaining) == 0: # if not left, sample a random word\n",
    "        return [random.choice(words)] ,lab\n",
    "    else:\n",
    "        return remaining,lab\n",
    "\n",
    "def random_swap(sentence,lab, n=5): \n",
    "    length = range(len(sentence)) \n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(length, 2)\n",
    "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
    "    return sentence,lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a80b4aa-40d7-4578-8f56-b9bbfd2e0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory, output_directory = 'stanfordSentimentTreebank','./';\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "for splitset, partition in partition(base_directory):\n",
    "    split_name = {1: \"train\", 2: \"test\", 3: \"dev\"}[splitset]\n",
    "    filename = os.path.join(output_directory, \"%s.csv\" % split_name)\n",
    "    del partition[\"splitset_label\"]\n",
    "    partition.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad49c112-5895-4f16-a051-c3d9cc98005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_label(label):\n",
    "    if label <= 0.05*100: return 'Class1'\n",
    "    if label <= 0.1*100: return 'Class2'\n",
    "    if label <= 0.15*100: return 'Class3'\n",
    "    if label <= 0.2*100: return 'Class4'\n",
    "    return 'Class5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a901fb9f-6f26-47bd-9bde-07f8d464ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "dev_data = pd.read_csv('dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d69a0c4-99d2-46b9-97b9-fb0f316e14bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df = pd.concat([train_data,test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14ad5d6-607a-445c-adc0-ba3903bcf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.reset_index(inplace = True,drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e1d46fa-b844-420a-bbc1-748fa146c37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10754, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd617876-cb85-4071-b5c4-0b91c43339fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pick(df):\n",
    "    for num in range (df.shape[0]):\n",
    "#     num = np.random.randint(0,df.shape[0])\n",
    "        return df.sentence[num],df.sentiment[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca78f3d4-4b91-4b4f-bc99-9dcf3fe1e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('all_transforms.csv',index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95595a34-caf0-48b9-a014-2e633aaeb83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the shape was :10754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1979/4205 [57:19<1:05:38,  1.77s/it]"
     ]
    }
   ],
   "source": [
    "# train_data.sentence\n",
    "# train_data.sentiment\n",
    "df = Train_df.copy()\n",
    "pbar = tqdm(range(5492+1057,df.shape[0]))\n",
    "count = len(df)\n",
    "print (f'Before the shape was :{len(df)}' )\n",
    "\n",
    "aug_data = []\n",
    "aug_label = []\n",
    "for i in pbar:\n",
    "    \n",
    "    \n",
    "    word,val = random_pick(df)\n",
    "    word1,val1 = back_translate(word,val)\n",
    "    \n",
    "    word,val = random_pick(df)\n",
    "    word = word.split()\n",
    "    word2,val2 = random_deletion(word,val)\n",
    "    word2 = ' '.join(i for i in word2)\n",
    "    \n",
    "    word,val = random_pick(df)\n",
    "    word = word.split()\n",
    "    word3,val3 = random_swap(word,val)\n",
    "    word3 = ' '.join(i for i in word3)\n",
    "     \n",
    "    ins = {'sentence':[word1,word2,word3],'label':[val1,val2,val3]}\n",
    "    df2 = pd.concat([df2,pd.DataFrame(ins)])\n",
    "    df2.to_csv('all_transforms_1.csv')\n",
    "pbar.set_description(desc = f'Loop:{i}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1822e7e-5b93-43bf-a9dc-806219dcafeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee132a-2b85-4cd6-bfb6-54e314c00377",
   "metadata": {},
   "outputs": [],
   "source": [
    "  0%|          | 0/4205 [00:00<?, ?it/s]\n",
    "Before the shape was :10754\n",
    " 30%|██▉       | 1258/4205 [36:24<1:25:02,  1.73s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989ec9a-00bd-4af6-ae91-64a23649ee05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
